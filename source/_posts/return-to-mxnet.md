---
title: 回归MXNET
date: 2018-02-01 12:19:44
tags: mxnet
category: 深度学习框架
---
mxnet最近几个月（2017/09起）都由[**沐神**](https://www.zhihu.com/people/li-mu-23/activities )在大力主推，除了在[**知乎**](https://www.zhihu.com/topic/20044209/hot) 大力宣传和建立[**技术讨论社区**](https://discuss.gluon.ai/ )外，更是亲自上阵，录制[**中文教程视频**](https://space.bilibili.com/209599371/#/video )。当然咯，mxnet的[**中文深度学习教程**](https://zh.gluon.ai/ )也比以前友好很多,其中最为欣赏是每个小章节大多都会留有一两道思考题，这比起其他的tutorials来说，更加灵活和具有启发性，相比之下，其他tutorials很容易给新手带来一种**“按部就班地学习”**的感受，这对于我这种ADHD患者而言，很快就会感到枯燥。
<!-- more -->

# 简单介绍下MXNET
（挖个坑，以后填）
社区里看到的关于优化问题的一个视频链接，google brain的大佬讲解的，就是速度有点慢：[传送门](http://videolectures.net/deeplearning2015_goodfellow_network_optimization/)

---

2/9/2018更新！

---

虽然说是简单介绍**MXNET**，但是我还是想从历史开始说起。那么就不得不提起[**DMLC**](https://github.com/dmlc)以下是**DMLC**的*“自我介绍”*：

>机器学习能从数据中学习。通常数据越多，能学习到的模型就越好。在数据获得越来越便利的今天，机器学习应用无论在广度上还是在深度上都有了显著进步。虽然近年来计算能力得到了大幅提高，但它仍然远远不及数据的增长和机器学习模型的复杂化。因此，机器学习算法速度和系统性能是目前工业界和学术界共同关心的热点。

>高性能和易用性的开源系统能对机器学习应用的其极大的推动作用。但我们发现目前兼具这两个特点的开源系统并不多，而且分散在各处。因此我们联合数个已有且被广泛使用的C++分布式机器学习系统的开发者，希望通过一个统一的组织来推动开源项目。我们为这个项目取名DMLC: Deep Machine Learning in Common，也可以认为是Distributed Machine Learning in C++。中文名为深盟。

目前**DMLC**的主要项目有：**xgboost,nnvm,ps-lite,mxnet,tvm,dmlc-core**等等

其中**MXNET**的前身就是**DMLC**的**CXXNET**，在技术上，**CXXNET**有如下两个亮点：

>**灵活的公式支持和极致的C++模板编程**
>追求速度极致的开发者通常使用C++来实现深度神经网络。但往往需要给每个神经网络的层和更新公式编写独立的CUDA kernel。很多以C++为核心的代码之所以没有向matlab/numpy那样支持非常灵活的张量计算，是因为因为运算符重载和临时空间的分配会带来效率的降低。
然而，cxxnet利用深盟的mshadow提供了类似matlab/numpy的编程体验，但同时保留了C++性能的高效性。其背后的核心思想是expression template，它通过模板编程技术将开发者写的公式自动展开成优化过的代码，避免重载操作符等带来的额外数据拷贝和系统消耗。另外，mshadow通过模板使得非常方便的讲代码切换到CPU还是GPU运行。

>**通用的分布式解决方案**
>在分布式深度神经网络中，我们既要处理一台机器多GPU卡，和多台机器多GPU卡的情况。然而后者的延迟和带宽远差于前者，因此需要对这种两个情形做不同的技术考虑。cxxnet采用mshadow-ps这样一个统一的参数共享接口，并利用接下来将要介绍Parameter Server实现了一个异步的通讯接口。其通过单机多卡和多机多卡采用不同的数据一致性模型来达到算法速度和系统性能的最佳平衡

而**MXNET**很好地继承了这些优点，于2015年九月面世。

>MXNet 是一个全功能，灵活可编程和高扩展性的深度学习框架，支持深度学习模型中的最先进技术，包括卷积神经网络（CNN）和长期短期记忆网络（LSTM）。MXNet 由学术界发起，包括数个顶尖大学的研究人员的贡献，这些机构包括华盛顿大学和卡内基梅隆大学。

李沐大神的亲口介绍：

>mxnet是cxxnet的下一代，目前实现了cxxnet所有功能，但借鉴了minerva/torch7/theano，加入更多新的功能。

>1. ndarray编程接口，类似matlab/numpy.ndarray/
2. torch.tensor。独有优势在于通过背后的engine可以在性能上和内存使用上更优。
3. symbolic接口。这个可以使得快速构建一个神经网络，和自动求导。
4. 更多binding 目前支持比较好的是python，马上会有julia和R。
5. 更加方便的多卡和多机运行。
6. 性能上更优。目前mxnet比cxxnet快40%，而且gpu内存使用少了一半。

尽管发布时间在目前的主流深度学习框架中算是比较早的，但是**MXNET**过去很长一段时间都不温不火，很重要的原因是其比较糟糕的文档和社区环境（貌似没什么社区额？）。而自从16年年底，**AMAZON**宣布选择**MXNET**作为主要的深度学习框架后，**MXNET**似乎迎来了春天。

>MXNet 是在卡内基梅隆大学中诞生的，它是我所看到的最完美的深度学习可扩展框架，它可以让计算机科学更加美好。让不同学科，不同工作的人们团结在一起。我们对亚马逊选择 MXNet 感到兴奋，MXNet 将由此变得更加强大。」卡内基梅隆大学计算机科学系主任 Andrew Moore 说道。

去年，也就是17年九月份开始，**MXNET**主要负责人**李沐**根据**MXNET**主要使用群体是国内用户这一特点，恰如其分地推出了一系列中文教程。除了在[**知乎**](https://www.zhihu.com/topic/20044209/hot) 大力宣传和建立[**技术讨论社区**](https://discuss.gluon.ai/ )外，更是亲自上阵，录制[**中文教程视频**](https://space.bilibili.com/209599371/#/video )。还有[**中文深度学习教程**](https://zh.gluon.ai/ )

仿佛**MXNET**的春天迫在眉睫。

# 为什么选择MXNET？

简而言之，感觉**MXNET**是**TensorFlow**与**PyTorch**的一个折衷。
(以后有时间再填坑)