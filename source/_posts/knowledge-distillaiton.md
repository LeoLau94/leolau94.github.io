---
title: 知识蒸馏
date: 2018-01-23 10:52:54
mathjax: true
tags: 模型压缩与加速
category: 学习
---
# 知识蒸馏
对于监督学习而言，同样复杂度的模型下，输入的监督信息越丰富，训练效果一般也就越好。因此，为了将复杂度更低的模型达到复杂度更高的模型的训练效果，可以使用更丰富的监督信息，这就是“知识蒸馏”的初始目的。

所谓“知识蒸馏”，其实是迁移学习的一种，其最终目的是将一个庞大复杂的模型所学习到的知识（更丰富的监督信息），通过一定的技术手段迁移到精简的小模型上，使得小模型能具有与大模型相近的性能。
<!-- more -->
两个关键点：1.如何提取知识；2.如何完成知识迁移。

# 相关研究
[Jimmy等人](http://papers.nips.cc/paper/5484-do-deep-nets-really-need-to-be-deep)认为，Softmax层的输入比类别标签包含了更多丰富的监督信息，因此可以使用它作为label来对小模型进行训练，所以训练问题转化为了一个回归问题：$$L(W,β)=\frac{1}{2T}\sum_{t}\|g(x^{(t)};W,\beta)-z^{(t)}\|$$

[Hinton等人](https://arxiv.org/abs/1503.02531)认为，Softmax层的输出更好，因为它包含了所有类别的预测概率，可以视为一种软标签。传统意义上的类别标签只给出一个类别的信息，没有包含各类之前的相关信息。而Softmax的输出则包含了这种类间关系：预测概率越接近，表示这两类越相似。他们使用了一个超参数\\(T\\)来控制预测概率的平滑度：$$q_i=\frac{\exp(z_i/T)}{\sum_j\exp(z_j/T)}$$

>其中\\(T\\)被称为“温度”，取值越大，预测概率分布也越平滑，通常取值为1。

为了获得更高的预测精度，还可以使用传统的类别标签进行修正。因此，最终的损失函数由两部分组成：

1. 由小模型的预测结果与大模型的“软标签”构成的交叉熵；
2. 由小模型的预测结果与大模型的传统类别标签构成的交叉熵。

这两部分最终进行加权组合，两者的重要程度可以通过权重进行调节。

[Luo等人](http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/11977/12130)认为，[Hinton等人](https://arxiv.org/abs/1503.02531)中温度\\(T\\)取值不容易确定，而且当类别较多时（例如人脸识别），由于“软标签”维度较高，模型训练难以收敛。因此，他们提出了，使用Softmax层前一层网络的输出作为小模型的标签来监督训练。这是因为该层的输出为Softmax层提供了信息但维度更少。不过，该层的输出也包含了许多噪声和无用的信息。因此，他们设计了一个算法来进行神经元选择，以去除这些无关维度。该算法主要思想是保留具有如下要求的特征维度：

1. 该维度的特征具有足够的区分度
2. 不同维度之间的相关性尽可能低